# -*- coding: utf-8 -*-
"""network_analysis_2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wI4evsP11XcNqdRa9qeZTYWX8lQPUwjp

# 연관어분석과 시각화
"""

# 나눔글꼴 설치
!sudo apt-get install -y fonts-nanum
!sudo fc-cache -fv
!rm ~/.cache/matplotlib -rf

# 한국어 처리 패키지 설치
!pip install konlpy

# 필요한 패키지 임포트
import networkx as nx
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
plt.rc('font', family='NanumBarunGothic') 
import konlpy
from konlpy.tag import *
from matplotlib import font_manager, rc 
warnings.filterwarnings('ignore')

# import signaturehelper

# import os, re, time, random , json
# import urllib
# import requests
# import warnings
# from nltk import Text
# #from wordcloud import WordCloud
# from tqdm import tqdm_notebook
# import platform
# import seaborn as sns 
# import scipy.stats as spst
# from statsmodels.graphics.mosaicplot import mosaic
# import statsmodels.api as sm

# path = "malgun.ttf"

# if platform.system() == "Darwin":
#     rc("font", family="Arial Unicode MS")
# elif platform.system() == "Windows":
#     font_name = font_manager.FontProperties(fname=path).get_name()
#     rc("font", family=font_name)
# else:
#     print("Unknown system. sorry")

# 크롤링 데이터 불러오기
df = pd.read_csv("tweet_temp.csv", encoding="cp949", engine="python")
df.head()

# 정규표현 처리를 위한 표준 라이브러리 re모듈 임포트
import re

# 텍스트 정제 함수 : 한글 이외의 문자는 전부 제거합니다.
def text_cleaning(text):
    hangul = re.compile('[^ ㄱ-ㅣ가-힣]+') # 한글의 정규표현식을 나타냅니다.
    result = hangul.sub('', text)
    return result

# ‘tweet_text’ 피처에 이를 적용합니다.
df['ko_text'] = df['tweet_text'].apply(lambda x: text_cleaning(x))
df.head()

# 한국어 처리 패키지 임포트
from konlpy.tag import Okt
from konlpy.tag import Hannanum
from collections import Counter

# 한국어 약식 불용어사전을 이용한 불용어 제거
korean_stopwords_path = "korean_stopwords.txt"
with open(korean_stopwords_path, encoding='utf8') as f:
    stopwords = f.readlines()
stopwords = [x.strip() for x in stopwords]

def get_nouns(x):
    nouns_tagger = Okt()
    nouns = nouns_tagger.nouns(x)
    
    # 한글자 키워드를 제거
    nouns = [noun for noun in nouns if len(noun) > 1]
    
    # 불용어를 제거
    nouns = [noun for noun in nouns if noun not in stopwords]
    
    return nouns

# ‘ko_text’ 피처에 이를 적용하여 키워드 추출하여 nouns컬럼에 저장
df['nouns'] = df['ko_text'].apply(lambda x: get_nouns(x))
print(df.shape)
df.head()

# 데이터 프레임에서 nouns컬럼만 남기고 모두 삭제
df.drop(columns=["created", "tweet_text", "ko_text"],inplace=True)

# 동시출현 단어쌍 (말뭉치) 도출
count = {}   #동시출현 빈도가 저장될 dict
for line in df['nouns']:
    #하나의 문서에서 동일한 단어가 두번 나와도 두번의 동시출현으로 고려X
    words = list(set(line))   
    #한줄씩 읽어와서 단어별로 분리(unique한 값으로 받아오기)
    #split은 띄어쓰기를 단어로 구분하라는 함수 
    
    for i, a in enumerate(words):
        for b in words[i+1:]:
            if a>b: 
                count[b, a] = count.get((b, a),0) + 1  
            else :
                count[a, b] = count.get((a, b),0) + 1

count.get(("a", "b"),0) #a, b라는 key가 없을 때는 디폴트를 0으로 해라 
count

#dictionary형 자료형을 판다스 데이터프레임으로 만들어줌 
#orient=index를 넣어야 행으로 쭉 나열이 됨 
df=pd.DataFrame.from_dict(count, orient='index')

list1=[]
for i in range(len(df)):
    #index를 중심으로 계속 중첩해서 list에 넣는다 
    list1.append([df.index[i][0],df.index[i][1],df[0][i]])

#pandas 이용해서 df형태로 만들기 
df2=pd.DataFrame(list1, columns=['word1','word2',"freq"])

#pandas 이용해서 sorting 하기 (디폴트가 오름차순이라서 false 꼭 써줘야 내림차순으로 나옴)
df3=df2.sort_values(by=['freq'],ascending=False)

#빈도 수 높은 순으로 말뭉치 10개만 가져오기
df3=df3.head(20)

#데이터프레임 CSV형태로 저장
df3
df3.to_csv("networkx1.csv")

# 연관어분석 시각회
# -*- encoding: utf-8 -*-
import pandas as pd
import networkx as nx
import operator
import numpy as np

if __name__ == '__main__':
    # 단어쌍 동시출현 빈도수를 담았던 networkx.csv파일을 불러온다.
    dataset = pd.read_csv("networkx1.csv")

    # 중심성 척도 계산을 위한 Graph를 만든다
    G_centrality = nx.Graph()

    # 빈도수가 높은 10개의 단어쌍에 대해서만 edge(간선)을 표현한다.
    for ind in range((len(dataset))):
        G_centrality.add_edge(dataset['word1'][ind], dataset['word2'][ind], weight=int(dataset['freq'][ind]))

    dgr = nx.degree_centrality(G_centrality)        # 연결 중심성
    btw = nx.betweenness_centrality(G_centrality)   # 매개 중심성
    cls = nx.closeness_centrality(G_centrality)     # 근접 중심성
    egv = nx.eigenvector_centrality(G_centrality)   # 고유벡터 중심성
    pgr = nx.pagerank(G_centrality)                 # 페이지 랭크

    # 중심성이 큰 순서대로 정렬한다.
    sorted_dgr = sorted(dgr.items(), key=operator.itemgetter(1), reverse=True)
    sorted_btw = sorted(btw.items(), key=operator.itemgetter(1), reverse=True)
    sorted_cls = sorted(cls.items(), key=operator.itemgetter(1), reverse=True)
    sorted_egv = sorted(egv.items(), key=operator.itemgetter(1), reverse=True)
    sorted_pgr = sorted(pgr.items(), key=operator.itemgetter(1), reverse=True)

    # 단어 네트워크를 그려줄 Graph 선언
    plt.figure(figsize=(10,10))
    G = nx.Graph()

    # 페이지 랭크에 따라 두 노드 사이의 연관성을 결정한다. (단어쌍의 연관성)
    # 연결 중심성으로 계산한 척도에 따라 노드의 크기가 결정된다. (단어의 등장 빈도수)
    for i in range(len(sorted_pgr)):
        G.add_node(sorted_pgr[i][0], nodesize=sorted_dgr[i][1])

    for ind in range((len(dataset))):
        G.add_weighted_edges_from([(dataset['word1'][ind], dataset['word2'][ind], int(dataset['freq'][ind]))])

    # 노드 크기 조정
    sizes = [G.nodes[node]['nodesize'] * 5000 for node in G]

    options = {
        'node_color': '#A7E4E4',
        'edge_color': '#0B8B8B',
        'width': 1,
        'with_labels': True,
        'font_weight': 'regular',
    }

    # 폰트 설정을 위한 font_manager import
    import matplotlib.font_manager as fm
    import matplotlib.pyplot as plt


    nx.draw(G, node_size=sizes, pos=nx.spring_layout(G, k=3.5, iterations=100), **options, font_family='NanumBarunGothic')  # font_family로 폰트 등록
    ax = plt.gca()
    ax.collections[0].set_edgecolor("#0B8B8B")
    plt.show()